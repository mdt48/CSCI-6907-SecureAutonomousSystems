{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trace_file(files, type, tqdm_desc):\n",
    "    columns =['spdX', 'spdY', 'accX', 'accY', 'posX', 'posY', 'hedX', 'hedY', 'spdXNoise', 'spdYNoise', 'accXNoise', 'accYNoise', 'posXNoise', 'posYNoise', 'hedXNoise', 'hedYNoise', 'label', 'messageID']\n",
    "    rows = []\n",
    "    ret_df = pd.DataFrame()\n",
    "    for file in tqdm(files, desc=tqdm_desc):\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                dec = json.JSONDecoder()\n",
    "                pos = 0\n",
    "                while not pos == len(str(line)):\n",
    "                    json_line, json_len = dec.raw_decode(str(line)[pos:])\n",
    "                    pos += json_len\n",
    "\n",
    "                    # json_line = json.loads(j)\n",
    "\n",
    "                    if json_line['type'] == type:\n",
    "                        label = 0 if 'A0' in file else 1\n",
    "                        new_row = (\n",
    "                            json_line['spd'][0],\n",
    "                            json_line['spd'][1],\n",
    "                            json_line['acl'][0],\n",
    "                            json_line['acl'][1],\n",
    "                            json_line['pos'][0],\n",
    "                            json_line['pos'][1],\n",
    "                            json_line['hed'][0],\n",
    "                            json_line['hed'][1],\n",
    "\n",
    "                            json_line['spd_noise'][0],\n",
    "                            json_line['spd_noise'][1],\n",
    "                            json_line['acl_noise'][0],\n",
    "                            json_line['acl_noise'][1],\n",
    "                            json_line['pos_noise'][0],\n",
    "                            json_line['pos_noise'][1],\n",
    "                            json_line['hed_noise'][0],\n",
    "                            json_line['hed_noise'][1],\n",
    "\n",
    "\n",
    "                            label, \n",
    "                            json_line['messageID'] if 'messageID' in json_line.keys() else -1\n",
    "                        )\n",
    "                        df = pd.DataFrame([list(new_row)], columns=columns)\n",
    "                        rows.append(df)\n",
    "    all_df = [ret_df]\n",
    "    all_df.extend(rows)\n",
    "    ret_df = pd.concat(all_df)\n",
    "    return ret_df\n",
    "                        # pd.concat(ret_df, df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating Training Data: 100%|██████████| 6298/6298 [04:02<00:00, 25.98it/s]\n",
      "Aggregating Testing Data:   3%|▎         | 124/4369 [00:23<08:16,  8.55it/s]"
     ]
    }
   ],
   "source": [
    "train_path = './data/train/*.*'\n",
    "train_files = glob(train_path)\n",
    "train_df = read_trace_file(train_files, 2, 'Aggregating Training Data')\n",
    "\n",
    "test_path = './data/test/*.*'\n",
    "test_files = glob(test_path)\n",
    "test_df = read_trace_file(test_files, 3, 'Aggregating Testing Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.loc[:, :'hedYNoise'].to_numpy()\n",
    "Y = train_df.loc[:, 'label'].to_numpy()\n",
    "\n",
    "X_test = test_df.loc[:, :'hedYNoise'].to_numpy()\n",
    "Y_test = test_df.loc[:, 'label'].to_numpy()\n",
    "message_ids = test_df.loc[:, 'messageID'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_shape,32)\n",
    "    self.fc2 = nn.Linear(32,64)\n",
    "    self.fc3 = nn.Linear(64,1)\n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.sigmoid(self.fc3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, loss_fn, trainloader, optimizer, x, y):\n",
    "    #forward loop\n",
    "    losses = []\n",
    "    accur = []\n",
    "    for i in range(epochs):\n",
    "        for j,(x_train,y_train) in enumerate(trainloader):\n",
    "            \n",
    "            #calculate output\n",
    "            output = model(x_train)\n",
    "        \n",
    "            #calculate loss\n",
    "            loss = loss_fn(output,y_train.reshape(-1,1))\n",
    "        \n",
    "            #accuracy\n",
    "            predicted = model(torch.tensor(x,dtype=torch.float32))\n",
    "            acc = (predicted.reshape(-1).detach().numpy().round() == y).mean()\n",
    "            #backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            losses.append(loss)\n",
    "            accur.append(acc)\n",
    "            print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))\n",
    "    return losses, accur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, x_test):\n",
    "    #forward loop\n",
    "    preds = []\n",
    "    for j,(x_test, y_test) in enumerate(testloader):\n",
    "        preds = model(torch.tensor(x_test,dtype=torch.float32))\n",
    "        preds.reshape(-1).detach().numpy().round()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Shit\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Instance and class checks can only be used with @runtime_checkable protocols",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mdt/Projects/CSCI-6907-SecureAutonomousSystems/mp3/NN.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mdt/Projects/CSCI-6907-SecureAutonomousSystems/mp3/NN.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_set \u001b[39m=\u001b[39m dataset(X, Y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mdt/Projects/CSCI-6907-SecureAutonomousSystems/mp3/NN.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_set \u001b[39m=\u001b[39m dataset(X_test, Y_test)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mdt/Projects/CSCI-6907-SecureAutonomousSystems/mp3/NN.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainloader \u001b[39m=\u001b[39m DataLoader(train_set,batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mdt/Projects/CSCI-6907-SecureAutonomousSystems/mp3/NN.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m testloader \u001b[39m=\u001b[39m DataLoader(train_set,batch_size\u001b[39m=\u001b[39mtest_set\u001b[39m.\u001b[39mlength,shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/sas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:200\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiprocessing_context \u001b[39m=\u001b[39m multiprocessing_context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Arg-check dataset related before checking samplers because we want to\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# tell users that iterable-style datasets are incompatible with custom\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# samplers first, so that they don't learn that this combo doesn't work\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# after spending time fixing the custom sampler errors.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(dataset, IterableDataset):\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m=\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[1;32m    202\u001b[0m     \u001b[39m# NOTE [ Custom Samplers and IterableDataset ]\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39m# `IterableDataset` does not support custom `batch_sampler` or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39m# this, and support custom samplers that specify the assignments to\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# specific workers.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/sas/lib/python3.10/typing.py:1497\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__instancecheck__\u001b[39m(\u001b[39mcls\u001b[39m, instance):\n\u001b[1;32m   1490\u001b[0m     \u001b[39m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m     \u001b[39m# assigned in __init__.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1493\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_runtime_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m         \u001b[39mnot\u001b[39;00m _allow_reckless_class_checks(depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   1496\u001b[0m     ):\n\u001b[0;32m-> 1497\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInstance and class checks can only be used with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1498\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39m @runtime_checkable protocols\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m ((\u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m             _is_callable_members_only(\u001b[39mcls\u001b[39m)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m             \u001b[39missubclass\u001b[39m(instance\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39mcls\u001b[39m)):\n\u001b[1;32m   1503\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Instance and class checks can only be used with @runtime_checkable protocols"
     ]
    }
   ],
   "source": [
    "train_set = dataset(X, Y)\n",
    "test_set = dataset(X_test, Y_test)\n",
    "\n",
    "trainloader = DataLoader(train_set,batch_size=64,shuffle=False)\n",
    "testloader = DataLoader(train_set,batch_size=test_set.length,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 700\n",
    "# Model , Optimizer, Loss\n",
    "model = Net(input_shape=x.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "losses, accur = train(epochs, model, loss_fn, trainloader, optimizer, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "#printing the accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accur)\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test(model, testloader, X_test)\n",
    "np.sum(preds) / len(preds)\n",
    "results = pd.DataFrame(columns=['data_id', 'prediction'])\n",
    "results['data_id'] = list(message_ids)\n",
    "results['prediction'] = list(preds)\n",
    "results.to_csv('results13.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sas2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6e7b6909d79b1d985a328ec253d73427af039940977619e6a3a5268017eb1ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
